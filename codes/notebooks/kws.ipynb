{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import importlib\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "import IPython\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../datasets')\n",
    "sys.path.append(os.path.join(os.getcwd(), '../models/'))\n",
    "\n",
    "import ai8x\n",
    "from kws20 import KWS\n",
    "import kws20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.FileIO name=65 mode='rb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/emretopcu/ai8x-training/venv/lib/python3.8/site-packages/audioread/__init__.py\", line 99, in available_backends\n",
      "    if ffdec.available():\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name=65>\n",
      "Exception ignored in: <_io.FileIO name=67 mode='rb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/emretopcu/ai8x-training/venv/lib/python3.8/site-packages/audioread/__init__.py\", line 99, in available_backends\n",
      "    if ffdec.available():\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name=67>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No key `stretch` in input augmentation dictionary! Using defaults: [Min: 0.8, Max: 1.3]\n",
      "Processing the label: backward. 1 of 36\n",
      "Finished in 0.358 seconds.\n",
      "Processing the label: bed. 2 of 36\n",
      "Finished in 0.324 seconds.\n",
      "Processing the label: bird. 3 of 36\n",
      "Finished in 0.333 seconds.\n",
      "Processing the label: cat. 4 of 36\n",
      "Finished in 0.325 seconds.\n",
      "Processing the label: dog. 5 of 36\n",
      "Finished in 0.339 seconds.\n",
      "Processing the label: down. 6 of 36\n",
      "Finished in 0.627 seconds.\n",
      "Processing the label: eight. 7 of 36\n",
      "Finished in 0.609 seconds.\n",
      "Processing the label: five. 8 of 36\n",
      "Finished in 0.656 seconds.\n",
      "Processing the label: follow. 9 of 36\n",
      "Finished in 0.253 seconds.\n",
      "Processing the label: forward. 10 of 36\n",
      "Finished in 0.250 seconds.\n",
      "Processing the label: four. 11 of 36\n",
      "Finished in 0.603 seconds.\n",
      "Processing the label: go. 12 of 36\n",
      "Finished in 0.624 seconds.\n",
      "Processing the label: happy. 13 of 36\n",
      "Finished in 0.331 seconds.\n",
      "Processing the label: house. 14 of 36\n",
      "Finished in 0.343 seconds.\n",
      "Processing the label: learn. 15 of 36\n",
      "Finished in 0.258 seconds.\n",
      "Processing the label: left. 16 of 36\n",
      "Finished in 0.616 seconds.\n",
      "Processing the label: librispeech. 17 of 36\n",
      "Finished in 3.358 seconds.\n",
      "Processing the label: marvin. 18 of 36\n",
      "Finished in 0.338 seconds.\n",
      "Processing the label: nine. 19 of 36\n",
      "Finished in 0.629 seconds.\n",
      "Processing the label: no. 20 of 36\n",
      "Finished in 0.632 seconds.\n",
      "Processing the label: off. 21 of 36\n",
      "Finished in 0.599 seconds.\n",
      "Processing the label: on. 22 of 36\n",
      "Finished in 0.617 seconds.\n",
      "Processing the label: one. 23 of 36\n",
      "Finished in 0.622 seconds.\n",
      "Processing the label: right. 24 of 36\n",
      "Finished in 0.609 seconds.\n",
      "Processing the label: seven. 25 of 36\n",
      "Finished in 0.645 seconds.\n",
      "Processing the label: sheila. 26 of 36\n",
      "Finished in 0.325 seconds.\n",
      "Processing the label: six. 27 of 36\n",
      "Finished in 0.623 seconds.\n",
      "Processing the label: stop. 28 of 36\n",
      "Finished in 0.624 seconds.\n",
      "Processing the label: three. 29 of 36\n",
      "Finished in 0.601 seconds.\n",
      "Processing the label: tree. 30 of 36\n",
      "Finished in 0.282 seconds.\n",
      "Processing the label: two. 31 of 36\n",
      "Finished in 0.624 seconds.\n",
      "Processing the label: up. 32 of 36\n",
      "Finished in 0.600 seconds.\n",
      "Processing the label: visual. 33 of 36\n",
      "Finished in 0.257 seconds.\n",
      "Processing the label: wow. 34 of 36\n",
      "Finished in 0.340 seconds.\n",
      "Processing the label: yes. 35 of 36\n",
      "Finished in 0.652 seconds.\n",
      "Processing the label: zero. 36 of 36\n",
      "Finished in 0.653 seconds.\n",
      "Training+Validation: 113472,  Test: 12979\n",
      "\n",
      "Processing test...\n",
      "Class up (# 31): 372 elements\n",
      "Class down (# 5): 403 elements\n",
      "Class left (# 15): 383 elements\n",
      "Class right (# 23): 380 elements\n",
      "Class stop (# 27): 391 elements\n",
      "Class go (# 11): 396 elements\n",
      "Class yes (# 34): 432 elements\n",
      "Class no (# 19): 425 elements\n",
      "Class on (# 21): 384 elements\n",
      "Class off (# 20): 393 elements\n",
      "Class one (# 22): 399 elements\n",
      "Class two (# 30): 379 elements\n",
      "Class three (# 28): 401 elements\n",
      "Class four (# 10): 382 elements\n",
      "Class five (# 7): 409 elements\n",
      "Class six (# 26): 411 elements\n",
      "Class seven (# 24): 410 elements\n",
      "Class eight (# 6): 397 elements\n",
      "Class nine (# 18): 413 elements\n",
      "Class zero (# 35): 425 elements\n",
      "Class UNKNOWN: 4994 elements\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/emretopcu/ai8x-training/data'\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, act_mode_8bit):\n",
    "        self.act_mode_8bit = act_mode_8bit\n",
    "        self.truncate_testset = False\n",
    "\n",
    "simulate = False\n",
    "args = Args(act_mode_8bit=simulate)\n",
    "\n",
    "_ ,test_set = kws20.KWS_20_get_datasets((data_path, args), load_train=False, load_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'{len(train_set)}')\n",
    "print(f'{len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_length = 16384\n",
    "#transform = torchaudio.transforms.TimeStretch(rate)\n",
    "#audio2 = transform(audio)\n",
    "#if len(audio2) > input_length:\n",
    "#    audio2 = audio2[:input_length]\n",
    "#else:\n",
    "#    audio2 = np.pad(audio2, (0, max(0, input_length - len(audio2))), \"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['up', 'down', 'left', 'right', 'stop', 'go', 'yes', 'no', 'on', 'off', 'one',\n",
    "                   'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
    "                   'UNKNOWN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio(data, target):\n",
    "    plt.plot(data)\n",
    "    plt.title(classes[target])\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(data_type):\n",
    "    if data_type == 'train':\n",
    "        sample_index = np.random.randint(0,len(train_set))\n",
    "\n",
    "        sample, target = train_set[sample_index]\n",
    "        print(f'target: {target}, sample index: {sample_index}')\n",
    "    if data_type == 'test':\n",
    "        sample_index = np.random.randint(0,len(test_set))\n",
    "\n",
    "        sample, target = test_set[sample_index]\n",
    "        print(f'target: {target}, sample index: {sample_index}')\n",
    "    return sample, target, sample_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_mixer2(audio, snr):\n",
    "    \"\"\"\n",
    "    Mix clean and noise with provided SNR level.\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    audio = torch.from_numpy(audio)\n",
    "    signal_pwr = torch.var(audio)\n",
    "    snr_linear = torch.tensor(10**(snr / 10), dtype=torch.float32)\n",
    "    n_var = signal_pwr / snr_linear\n",
    "    noise = torch.randn(audio.shape) * torch.sqrt(n_var)\n",
    "    noisy_signal = audio + noise\n",
    "    max_abs_noise = torch.max(torch.abs(noisy_signal))\n",
    "\n",
    "    # Normalize the noise tensor\n",
    "    if max_abs_noise > 1:\n",
    "        noisy_signal /= max_abs_noise\n",
    "    \n",
    "    return noisy_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_low = torch.tensor(20, dtype=torch.int32)\n",
    "snr_high = torch.tensor(51, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_a = os.path.join(\"..\", \"..\" ,\"ai8x-synthesis\", \"trained\", \"ai85-kws20-qat8.pth.tar\")\n",
    "strategy_b = os.path.join(\"..\", \"logs\", \"2023.08.22-164553\", \"qat_best.pth.tar\")\n",
    "strategy_c = os.path.join(\"..\", \"logs\", \"yeni_yeni_merve\", \"qat_best.pth.tar\")\n",
    "mod = importlib.import_module(\"ai85net-kws20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Working with device:\", device)\n",
    "\n",
    "ai8x.set_device(device=85, simulate=False, round_avg=False)\n",
    "qat_policy = {'start_epoch': 10, 'weight_bits': 8, 'bias_bits': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mod.AI85KWS20Net(num_classes=len(classes), num_channels=128, dimensions=(128, 1), bias=False, \n",
    "                           quantize_activation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(strategy_a)\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "ai8x.update_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, true_labels):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_data = torch.from_numpy(test_data).float().to(device) \n",
    "    true_labels = true_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_data)\n",
    "    \n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    print(predicted_classes[2500:2800])\n",
    "\n",
    "    accuracy = (predicted_classes == true_labels).float().mean()\n",
    "    print(f\"Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(model, test_data):\n",
    "    model.eval()\n",
    "\n",
    "    #test_data = torch.from_numpy(test_data).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for validation_step, (inputs, targets) in enumerate(test_set):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_data)\n",
    "    print(test_data[0])\n",
    "    print(test_data[0].shape)\n",
    "    \n",
    "    \n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    #print(predicted_classes[2300:2800])\n",
    "\n",
    "    accuracy = (predicted_classes == true_labels).float().mean()\n",
    "    print(f\"Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchnet.meter as tnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "classerr = tnt.ClassErrorMeter(accuracy=True, topk=(1, min(len(classes), 5)))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for validation_step, (inputs, targets) in enumerate(test_set):\n",
    "        #inputs = snr_mixer2(inputs,50)\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        inputs = inputs.to(device)\n",
    "    \n",
    "        \n",
    "        # compute output from model\n",
    "        output = model(inputs)\n",
    "        classerr.add(output, targets)\n",
    "        if validation_step % 1000 == 0:\n",
    "            print(\"Batch\",validation_step)\n",
    "            print(\"Accuracy:\", classerr.value()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", classerr.value()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", classerr.value()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[4000:4500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "classerr = tnt.ClassErrorMeter(accuracy=True, topk=(1, min(len(classes), 5)))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for validation_step, (inputs, targets) in enumerate(test_set):\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        inputs = inputs.to(device)\n",
    "    \n",
    "        \n",
    "        # compute output from model\n",
    "        output = model(inputs)\n",
    "        classerr.add(output, targets)\n",
    "        if validation_step % 1000 == 0:\n",
    "            print(\"Batch\",validation_step)\n",
    "            print(\"Accuracy:\", classerr.value()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", classerr.value()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Whole dataset contains---\",len(test_set),\"---samples.\")\n",
    "print(\"Each sample is a tuple of---\",len(test_set[0]),\"---elements.\")\n",
    "print(\"Tuple's first element contains height of each element is---\",len(test_set[0][0]),\"---long.\")\n",
    "print(\"Width of each element is---\",len(test_set[0][0][0]),\"---long.\")\n",
    "print(\"Tuple's second element contains labels---\",test_set[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in range(snr_low, snr_high, 1):\n",
    "\n",
    "    len_noisy_test_set = (len(test_set), 128, 128)\n",
    "    noisy_test_set = np.zeros(len_noisy_test_set)\n",
    "\n",
    "    for i, signal in enumerate(test_set):\n",
    "        noisy = snr_mixer2(signal[0], db)\n",
    "        noisy_test_set[i] = noisy\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    break\n",
    "        \n",
    "    evaluate(model, noisy_test_set,test_set[0][1])\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list comprehension to extract the labels\n",
    "labels = [label for _, label in test_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_samples = [sample for sample, _ in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_tensor = torch.stack(test_set_samples)\n",
    "test_set_samples = stacked_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(labels, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[2500:2800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_set_samples, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, noisy_test_set, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_set[0][0][127]))\n",
    "print(test_set[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, sample_index = random_sample('test')\n",
    "\n",
    "audio_sample = np.array(sample)\n",
    "audio_sample = audio_sample.flatten('F')\n",
    "plot_audio(audio_sample, target)\n",
    "print(len(audio_sample))\n",
    "print(audio_sample.shape)\n",
    "print(type(audio_sample))\n",
    "print(max(audio_sample))\n",
    "print(np.mean(audio_sample))\n",
    "\n",
    "#7925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, sample_index = random_sample('test')\n",
    "audio_sample = np.array(test_set[5000][0])\n",
    "\n",
    "audio_sample_noisy = snr_mixer2(audio_sample, 3)\n",
    "\n",
    "audio_sample_noisy = np.array(audio_sample_noisy)\n",
    "\n",
    "print(audio_sample_noisy.shape)\n",
    "print(audio_sample.shape)\n",
    "audio_sample = audio_sample.flatten('F')\n",
    "audio_sample_noisy = audio_sample_noisy.flatten('F')\n",
    "plot_audio(audio_sample, test_set[sample_index][1])\n",
    "plot_audio(audio_sample_noisy, test_set[sample_index][1])\n",
    "#audio_sample_noisy\n",
    "#print(len(audio_sample))\n",
    "print(audio_sample.shape)\n",
    "#print(type(audio_sample))\n",
    "##print(max(audio_sample))\n",
    "#print(np.mean(audio_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write('/home/emretopcu/ai8x-training/notebooks/test.wav', 16384, audio_sample)\n",
    "IPython.display.Audio('/home/emretopcu/ai8x-training/notebooks/test.wav', rate = 16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write('/home/emretopcu/ai8x-training/notebooks/test.wav', 16384, audio_sample_noisy)\n",
    "IPython.display.Audio('/home/emretopcu/ai8x-training/notebooks/test.wav', rate = 16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
